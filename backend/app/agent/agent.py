from typing import Dict, List, Any, AsyncIterator
import os
import json

from langgraph.graph import StateGraph, END
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate

from app.agent.tools import (
    generate_explanations_for_topic,
    search_youtube_videos,
    generate_mermaid_for_topic,
)

# -------------------------------------------------------------------
# HARD REQUIREMENTS (AI-ONLY MODE)
# -------------------------------------------------------------------

if not os.getenv("GROQ_API_KEY"):
    raise RuntimeError("GROQ_API_KEY is required to run the course generation agent.")

llm = ChatGroq(
    model="llama-3.3-70b-versatile",
    temperature=0.3
)

# -------------------------------------------------------------------
# STATE
# -------------------------------------------------------------------

class CourseState(dict):
    prompt: str
    enhanced_prompt: str
    topics: List[str]
    # Iterative state
    pending_topics: List[str]
    generated_modules: Dict[str, Any]
    # Final output
    course: Dict[str, Any]


# -------------------------------------------------------------------
# NODE 1: Enhance Prompt → Course Title
# -------------------------------------------------------------------


def node_enhance_prompt(state: CourseState) -> CourseState:
    prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a senior instructional designer."),
        ("human", "Generate a concise professional course title (max 12 words) for: {prompt}")
    ])

    chain = prompt | llm
    resp = chain.invoke({"prompt": state["prompt"]})

    state["enhanced_prompt"] = resp.content.strip()
    return state

# -------------------------------------------------------------------
# NODE 2: Generate Course Topics (Modules)
# -------------------------------------------------------------------

def node_generate_topics(state: CourseState) -> CourseState:
    prompt = ChatPromptTemplate.from_messages([
        ("system", "You are an expert curriculum architect. Return JSON only."),
        ("human", """
        Design a complete course for "{title}".

        Requirements:
        - 8–12 modules
        - Progressive difficulty
        - Industry-relevant
        - Each module should be clearly distinct
        - Return ONLY a JSON array of strings
        """)
    ])

    chain = prompt | llm
    resp = chain.invoke({"title": state["enhanced_prompt"]})

    topics = json.loads(resp.content)

    if not isinstance(topics, list) or not topics:
        raise ValueError("Invalid topics generated by AI")

    state["topics"] = topics
    state["pending_topics"] = topics[:]  # Initialize pending
    state["generated_modules"] = {}      # Initialize storage
    return state


# -------------------------------------------------------------------
# HELPER: Generate Submodules per Topic
# -------------------------------------------------------------------

def generate_subtopics(topic: str) -> List[str]:
    prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a subject matter expert. Return JSON only."),
        ("human", """
        For the course module "{topic}", generate 4–6 detailed submodules.
        Each submodule should represent a clear learning unit.
        Return ONLY a JSON array of short strings.
        """)
    ])

    chain = prompt | llm
    resp = chain.invoke({"topic": topic})

    subtopics = json.loads(resp.content)

    if not isinstance(subtopics, list) or not subtopics:
        raise ValueError(f"Invalid subtopics for module: {topic}")

    return subtopics

# -------------------------------------------------------------------
# NODE 3: Generate Module Content (Iterative)
# -------------------------------------------------------------------

def node_generate_module(state: CourseState) -> CourseState:
    
    if not state["pending_topics"]:
        return state
    
    current_topic = state["pending_topics"].pop(0)
    
    
    subtopics = generate_subtopics(current_topic)
    explanations = generate_explanations_for_topic(current_topic, subtopics)
    videos = search_youtube_videos(f"{current_topic} tutorial", limit=3)
    mermaid = generate_mermaid_for_topic(current_topic, list(explanations.keys()))
    
    module_data = {
        "module_title": current_topic,
        "explanations": explanations,    # Restore dictionary for frontend
        "videos": videos,                # Rename back to 'videos'
        "mermaid": mermaid               # Rename back to 'mermaid'
    }
    
    # Store in state
    state["generated_modules"][current_topic] = module_data
    return state


# -------------------------------------------------------------------
# NODE 4: Finalize
# -------------------------------------------------------------------

def node_finalize_course(state: CourseState) -> CourseState:
    ordered_modules = {}
    for topic in state["topics"]:
        if topic in state["generated_modules"]:
            ordered_modules[topic] = state["generated_modules"][topic]

    state["course"] = {
        "title": state["enhanced_prompt"],
        "modules": ordered_modules
    }
    return state


# -------------------------------------------------------------------
# CONDITIONAL EDGES
# -------------------------------------------------------------------

def should_continue(state: CourseState) -> str:
    if state["pending_topics"]:
        return "generate_module"
    return "finalize_course"


# -------------------------------------------------------------------
# GRAPH BUILDER
# -------------------------------------------------------------------

def build_graph():
    graph = StateGraph(CourseState)

    graph.add_node("enhance_prompt", node_enhance_prompt)
    graph.add_node("generate_topics", node_generate_topics)
    graph.add_node("generate_module", node_generate_module)
    graph.add_node("finalize_course", node_finalize_course)

    graph.set_entry_point("enhance_prompt")
    graph.add_edge("enhance_prompt", "generate_topics")
    graph.add_edge("generate_topics", "generate_module")
    
    graph.add_conditional_edges(
        "generate_module",
        should_continue,
        {
            "generate_module": "generate_module",
            "finalize_course": "finalize_course"
        }
    )
    graph.add_edge("finalize_course", END)

    return graph.compile()


# -------------------------------------------------------------------
# PUBLIC API (Streaming)
# -------------------------------------------------------------------

async def run_workflow_stream(user_prompt: str) -> AsyncIterator[Dict[str, Any]]:
    workflow = build_graph()
    initial_state = {"prompt": user_prompt}
    
    # We'll stream the updates from the graph
    # stream_mode="updates" yields only the updates returned by the nodes
    async for event in workflow.astream(initial_state, stream_mode="updates"):
        yield event
